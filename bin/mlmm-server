#!/usr/bin/env python

######################################################################
# ML/MM: https://github.com/lohedges/sander-mlmm
#
# Copyright: 2023
#
# Authors: Lester Hedges   <lester.hedges@gmail.com>
#          Kirill Zinovjev <kzinovjev@gmail.com>
#
# ML/MM is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 2 of the License, or
# (at your option) any later version.
#
# ML/MM is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with ML/MM. If not, see <http://www.gnu.org/licenses/>.
######################################################################

import argparse
import os

from glob import glob

from mlmm.mlmm import MLMMCalculator
from mlmm.socket import Socket

# Check whether any MLMM environment variables are set.
MLMM_HOST = os.getenv("MLMM_HOST")
try:
    MLMM_PORT = int(os.getenv("MLMM_PORT"))
except:
    MLMM_PORT = None
MLMM_MODEL = os.getenv("MLMM_MODEL")
MLMM_EMBEDDING = os.getenv("MLMM_EMBEDDING")
try:
    MLMM_NUM_CLIENTS = int(os.getenv("MLMM_NUM_CLIENTS"))
except:
    MLMM_NUM_CLIENTS = None
MLMM_BACKEND = os.getenv("MLMM_BACKEND")
MLMM_DEVICE = os.getenv("MLMM_DEVICE")
MLMM_LOG = os.getenv("MLMM_LOG", "True").lower() in ("true", "1", "t")
DEEPMD_MODEL = os.getenv("DEEPMD_MODEL")
RASCAL_MODEL = os.getenv("RASCAL_MODEL")
RASCAL_PARM7 = os.getenv("RASCAL_PARM7")

# Fallback to default values.
if not MLMM_HOST:
    MLMM_HOST = "localhost"
if not MLMM_PORT:
    MLMM_PORT = 10000
if not MLMM_NUM_CLIENTS:
    MLMM_NUM_CLIENTS = 1
if not MLMM_BACKEND:
    MLMM_BACKEND = "torchani"
if not MLMM_EMBEDDING:
    MLMM_EMBEDDING = "electrostatic"
if not MLMM_DEVICE:
    MLMM_DEVICE = None


def validate_clients(num_clients):
    try:
        num_clients = int(num_clients)
    except:
        raise argparse.ArgumentTypeError(
            f"--num-clients: invalid int value: '{num_clients}'"
        )

    if num_clients < 0:
        raise argparse.ArgumentTypeError("--num-clients: Value must be >= 0")


# create argument parser.
parser = argparse.ArgumentParser(description="ML/MM server")

# parse command-line options
parser.add_argument("--host", type=str, help="the hostname.", required=False)
parser.add_argument("--port", type=str, help="the port number", required=False)
parser.add_argument(
    "--model", type=str, help="path to an ML/MM model file", required=False
)
parser.add_argument(
    "--embedding",
    type=str,
    help="the embedding method to use",
    choices=["electrostatic", "mechanical"],
    required=False,
)
parser.add_argument(
    "--num-clients",
    type=validate_clients,
    help="the maximum number of client connections to allow",
    required=False,
    default=1,
)
parser.add_argument(
    "--backend",
    type=str,
    help="the in vacuo backend",
    choices=["deepmd", "rascal", "torchani", "orca"],
    required=False,
)
parser.add_argument(
    "--device",
    type=str,
    help="the device to be used by PyTorch",
    choices=["cpu", "cuda"],
    required=False,
)
parser.add_argument(
    "--deepmd-model",
    type=str,
    nargs="+",
    help="path to DeepMD model file(s)",
    required=False,
)
parser.add_argument(
    "--rascal-model",
    type=str,
    nargs="+",
    help="path to Rascal model file",
    required=False,
)
parser.add_argument(
    "--rascal-parm7",
    type=str,
    nargs="+",
    help="path to the parm7 file used to train the Rascal model",
    required=False,
)
parser.add_argument(
    "--log",
    action=argparse.BooleanOptionalAction,
    help="whether to log energies",
    required=False,
)
args = parser.parse_args()

# Overload using command-line arguments.
if args.host:
    MLMM_HOST = args.host
if args.port:
    MLMM_PORT = args.port
if args.model:
    MLMM_MODEL = args.model
if args.embedding:
    MLMM_EMBEDDING = args.embedding
if args.num_clients:
    MLMM_NUM_CLIENTS = args.num_clients
if args.backend:
    MLMM_BACKEND = args.backend
if args.device:
    MLMM_DEVICE = args.device
if args.deepmd_model:
    DEEPMD_MODEL = args.deepmd_model
if args.rascal_model:
    RASCAL_MODEL = args.rascal_model
if args.rascal_parm7:
    RASCAL_PARM7 = args.rascal_parm7
if args.log is not None:
    MLMM_LOG = args.log

# Work out the DeePMD model. We allow the following formatting:
#    1) A path to a single file.
#    2) Multiple files specified using wildcards.
#    3) A list of files, comma-separated.
#
# The first two can be found using glob, if empty then try splitting on ",".
if DEEPMD_MODEL:
    if isinstance(DEEPMD_MODEL, list) and len(DEEPMD_MODEL) == 1:
        DEEPMD_MODEL = DEEPMD_MODEL[0]
    if isinstance(DEEPMD_MODEL, str):
        # Remove whitespace.
        DEEPMD_MODEL = DEEPMD_MODEL.replace(" ", "")

        # Try globbing.
        models = glob(DEEPMD_MODEL)

        # No matches.
        if not models:
            models = DEEPMD_MODEL.split(",")

            # Invalid formatting.
            if len(models) == 1:
                raise ValueError(
                    f"DeePMD model not found, or invalid formatting: '{DEEPMD_MODEL}'"
                )
            else:
                DEEPMD_MODEL = models
        else:
            DEEPMD_MODEL = models

print(f"Starting ML-MM server using {MLMM_BACKEND} backend...")

# Create the ML/MM socket.
sock = Socket()

try:
    sock.bind(MLMM_HOST, MLMM_PORT)
    sock.listen(MLMM_NUM_CLIENTS)

    # Write the PID to file in the current directory.
    with open("mlmm_pid.txt", "w") as f:
        f.write(f"{os.getpid()}\n")

    # Write the port to file in the current directory.
    with open("mlmm_port.txt", "w") as f:
        f.write(f"{MLMM_PORT}\n")
except:
    raise OSError(
        f"Server address already in use: ({MLMM_HOST}, {MLMM_PORT})"
    ) from None

# Create the ML/MM calculator.
print("Initialising ML/MM calculator...")
mlmm_calculator = MLMMCalculator(
    model=MLMM_MODEL,
    embedding=MLMM_EMBEDDING,
    backend=MLMM_BACKEND,
    deepmd_model=DEEPMD_MODEL,
    rascal_model=RASCAL_MODEL,
    rascal_parm7=RASCAL_PARM7,
    device=MLMM_DEVICE,
    log=MLMM_LOG,
)

while True:
    print("Waiting for a connection...")
    connection, client_address = sock.accept()
    try:
        print("Client connected:", client_address)
        while True:
            msg, path = connection.receive()
            if msg == "mlmmrun":
                # Try to run the ML/MM calculation.
                print("Running ML/MM calculation...")
                try:
                    mlmm_calculator.run(path=path)
                    msg = "7:mlmmfin"
                    print("Finished!")
                except Exception as e:
                    msg = f"mlmmfail:{str(e)}"
                    msg = f"{len(msg)}:{msg}"
                    print("Failed!")

                # Tell the client that we have finished.
                connection.send(msg, len(msg))
            else:
                break
    finally:
        connection.close()
